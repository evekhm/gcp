#! /bin/bash

##########################################################################################
# This script deploys Cloud Function to Retrieve REST API Data and put into the BigQuery.
##########################################################################################
set -u # This prevents running the script if any of the variables have not been set
set -e # Exit if error is detected during pipeline execution
DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
print="$DIR/print"

if [ -z ${APPLICATION+x} ]   || [ -z ${BIGQUERY_DATASET+x} ]  || \
            [ -z  ${SOURCE_DIR+x} ] || [ -z  ${RUNTIME+x} ]  ||  \
            [ -z  ${INTERVAL+x} ]  || [ -z ${SOURCE_ENTRY_POINT+x} ]; then
  $print "Missing ENV paramters - APPLICATION BIGQUERY_DATASET SOURCE_DIR RUNTIME INTERVAL" 'ERROR'
  echo "Make sure to run 'source $DIR/SET_cgm' and check settings there"
  exit 1
fi

#Using info from active configuration
PROJECT_ID=$(gcloud config get-value project 2> /dev/null);
if [ -z "$PROJECT_ID" ]; then
  $print "Make sure active configuration has PROJECT set" 'ERROR'
  exit 1
fi
######################Variables based on APPLICATION Settings #############################
REGION=$(gcloud config get-value compute/region 2> /dev/null);
BUCKET="gs://${PROJECT_ID}-${APPLICATION}"
SERVICE_ACCOUNT=${SERVICE_ACCOUNT_NAME}@${PROJECT_ID}.iam.gserviceaccount.com
###############################################################################

create_pubsub_topic(){
  TOPIC=$1
  if gcloud pubsub topics list 2>/dev/null | grep "$TOPIC"; then
    $print "Topic [$TOPIC] already exists - skipping step" INFO
  else
    $print "Creating topic [$TOPIC]..." INFO
    gcloud pubsub topics create "$TOPIC"
  fi

  $print "Adding required permissions for service accounts" INFO
  gcloud pubsub topics add-iam-policy-binding "$TOPIC_ID" \
      --member="serviceAccount:$SERVICE_ACCOUNT"\
      --role="roles/pubsub.editor"
  gcloud pubsub topics add-iam-policy-binding "$TOPIC_ID" \
      --member="serviceAccount:$SERVICE_ACCOUNT"\
      --role="roles/pubsub.publisher"
}

create_pubsub_subscription(){
  SUBSCRIPTION=$1
  TOPIC=$2
  if gcloud pubsub subscriptions list 2>/dev/null | grep "$SUBSCRIPTION"; then
    $print "Dropping subscription [$SUBSCRIPTION] to avoid processing of old messages..." INFO
    "$DIR"/clean -subscription="$SUBSCRIPTION"
  fi

  $print "Creating subscription [$SUBSCRIPTION]..." INFO
  gcloud pubsub subscriptions create "$SUBSCRIPTION" --topic "$TOPIC"
}

create_gcs_bucket(){
  if gsutil ls | grep "${BUCKET}"; then
      $print "Bucket [$BUCKET] already exists - skipping step" INFO
  else
      echo "Creating GCS bucket for pipeline: [$BUCKET]..."
      gsutil mb -p "$PROJECT_ID" "${BUCKET}"/
  fi

  gsutil iam ch serviceAccount:"${SERVICE_ACCOUNT}":objectViewer "${BUCKET}"

#  gcloud projects add-iam-policy-binding "$PROJECT_ID" \
#    --member="serviceAccount:$SA_DATAFLOW" \
#    --role="roles/storage.admin"
}

deploy_cloud_function(){
  gcloud functions deploy monitor_pubsub \
      --trigger-topic ${PUBSUB_COMMAND_TOPIC} \
      --runtime $RUNTIME --source="${SOURCE_DIR}" \
      --entry-point=$SOURCE_ENTRY_POINT \
      --service-account=$SERVICE_ACCOUNT

}

create_bq_dt(){
  if bq --location="${REGION}" ls | grep  "${BIGQUERY_DATASET}"; then
    $print "Dataset already exists [${BIGQUERY_DATASET}] - skipping step" INFO
  else
    bq --location="${REGION}" mk --dataset "${BIGQUERY_DATASET}"
    bq mk --table "${BIGQUERY_DATASET}"."${BIGQUERY_TABLE}" message:STRING,userId:STRING,type:STRING
  fi
  bq add-iam-policy-binding \
      --member="serviceAccount:$SERVICE_ACCOUNT"\
      --role="roles/bigquery.dataEditor" \
      "${PROJECT_ID}:${BIGQUERY_DATASET}.${BIGQUERY_TABLE}"
}

deploy_cloud_scheduler(){
  #Creating App Engine Application (Required for the Scheduler)
  if gcloud app describe 2>&1 >/dev/null | grep 'does not contain an App Engine application' > /dev/null; then
      echo "GAE Default App not found - initializing AppEngines on the project..."
      gcloud app create --region="$REGION"
      LOCATION="$REGION"
  else
      LOCATION=$(gcloud compute regions list --format "value(name)" | grep "$(gcloud app describe --format "value(locationId)")")
  fi

  #  Any resource that needs App Engine can only be created/updated in the App Engine region.

  if gcloud scheduler jobs list --format "value(ID)" 2>/dev/null | grep "$SCHEDULER_JOB"; then
    $print "Job $SCHEDULER_JOB already exists - skipping step" INFO
  else
    gcloud scheduler jobs create pubsub "$SCHEDULER_JOB" \
        --topic="projects/$PROJECT_ID/topics/$PUBSUB_COMMAND_TOPIC" \
        --attributes="userId=UserDemo,topic=$TOPIC_ID" \
        --schedule="*/$INTERVAL * * * *" \
        --message-body="Triggering Cloud Function every $INTERVAL minutes" \
        --description="Continuous Pulling of $APPLICATION data" \
        --time-zone='UTC' \
        --location="$LOCATION"
  fi
}

deploy_dataflow(){
  #Cancel job if already exists
  exists=$(gcloud dataflow jobs list \
          --filter "NAME:$DATAFLOW_JOB AND STATE=Running" \
          --format 'value(JOB_ID)' \
          --region "$REGION" 2>/dev/null)

  if [[ -n $exists ]]; then
    #Cancel Previous Job
      $print "Cancelling previously active job=[$DATAFLOW_JOB]" INFO
      gcloud dataflow jobs list \
          --filter "NAME:$DATAFLOW_JOB AND STATE=Running" \
          --format 'value(JOB_ID)' \
          --region "$REGION" 2>/dev/null \
         | xargs gcloud dataflow jobs cancel --region "$REGION"

      sleep 10
  fi

  canceling=$(gcloud dataflow jobs list \
        --filter "NAME:$DATAFLOW_JOB AND STATE=Cancelling" \
        --format 'value(JOB_ID)' 2>/dev/null \
        --region "$REGION")

  if [ -z "$canceling" ]; then
      gcloud dataflow jobs run "$DATAFLOW_JOB" \
      --gcs-location gs://dataflow-templates-"${REGION}"/latest/PubSub_Subscription_to_BigQuery \
      --staging-location "$BUCKET/dataflow" \
      --region "${REGION}" \
      --service-account-email="${SERVICE_ACCOUNT}" \
      --max-workers 1 --num-workers 1 \
      --parameters inputSubscription=projects/"$PROJECT_ID"/subscriptions/${SUBSCRIPTION_ID},outputTableSpec="$PROJECT_ID":${BIGQUERY_DATASET}.${BIGQUERY_TABLE}
  else
        $print 'Will not start the new job, since previous is still being cancelled. Please, wait, and try again ' WARNING
  fi

}

create_service_account() {

  #gcloud iam service-accounts list --filter="EMAIL=${SERVICE_ACCOUNT}"
  if gcloud iam service-accounts list --project $PROJECT_ID | grep -q $SERVICE_ACCOUNT_NAME; then
    $print "Service account $SERVICE_ACCOUNT_NAME has been found." INFO
  else
    echo "Creating service account... $SERVICE_ACCOUNT_NAME"
    gcloud iam service-accounts create $SERVICE_ACCOUNT_NAME \
        --description="Runs $APPLICATION jobs" \
        --display-name="$APPLICATION-service-account"
  fi

  echo "Grant $APPLICATION_ROLE role to the service account $SERVICE_ACCOUNT_NAME ..."
  gcloud projects add-iam-policy-binding $PROJECT_ID \
      --member="serviceAccount:$SERVICE_ACCOUNT" \
      --role="projects/$PROJECT_ID/roles/$APPLICATION_ROLE"

  gcloud projects add-iam-policy-binding "$PROJECT_ID" \
    --member="serviceAccount:$SERVICE_ACCOUNT" \
    --role="roles/dataflow.worker"

}

create_role() {
  if gcloud iam roles list --project $PROJECT_ID 2>/dev/null | grep -q $APPLICATION_ROLE; then
    $print "Role [$APPLICATION_ROLE] already exists - updating it..." INFO
    ACTION="update"
  else
    echo "Creating a role [$APPLICATION_ROLE]..."
    ACTION="create"
  fi

  PERMISSIONS="dataflow.jobs.get,\
dataflow.jobs.get,\
logging.logEntries.create,\
storage.objects.create,\
bigquery.datasets.get,\
dataflow.jobs.get,\
logging.logEntries.create,\
pubsub.subscriptions.consume,\
pubsub.subscriptions.get,\
compute.projects.get,\
pubsub.subscriptions.consume,\
pubsub.subscriptions.get,\
pubsub.subscriptions.list,\
pubsub.topics.get,\
pubsub.topics.list,\
pubsub.topics.publish,\
pubsub.topics.update,\
storage.buckets.get,\
storage.buckets.list,\
storage.objects.create,\
storage.objects.get,\
storage.objects.list,\
storage.objects.update"

  gcloud iam roles ${ACTION} $APPLICATION_ROLE \
    --title "$APPLICATION Role" \
    --description "Access to resources needed to develop and deploy $APPLICATION" \
    --stage "GA" \
    --project $PROJECT_ID \
    --permissions "${PERMISSIONS}"

}

##############################################################################
#                         Flow to Deploy Resources
##############################################################################
$print "Creating Role=[$APPLICATION_ROLE]"
create_role
$print "Creating Service Account=[$SERVICE_ACCOUNT_NAME] binded to [$APPLICATION_ROLE] role"
create_service_account

$print "Creating Topic=[$TOPIC_ID] and subscription=[$SUBSCRIPTION_ID] for Receiving Data from device ..."
create_pubsub_topic "$TOPIC_ID"
create_pubsub_subscription "$SUBSCRIPTION_ID" "$TOPIC_ID"

$print "Creating Topic=[$PUBSUB_COMMAND_TOPIC] to Trigger Cloud Function..."
create_pubsub_topic "$PUBSUB_COMMAND_TOPIC"

$print "Deploying Cloud Function=[$SOURCE_ENTRY_POINT] to access [$APPLICATION] API upon Topic=[$PUBSUB_COMMAND_TOPIC]"
$print "Retrieved data will be published as a message into [$TOPIC_ID] topic" INFO
deploy_cloud_function

$print "Creating Cloud Scheduler to trigger [$PUBSUB_COMMAND_TOPIC] topic every $INTERVAL (min)..."
deploy_cloud_scheduler

$print "Creating Cloud Storage bucket=[$BUCKET] for DataFlow ..."
create_gcs_bucket

$print "Creating BigQuery Dataset=${BIGQUERY_DATASET} and Table=${BIGQUERY_TABLE}for the received data..."
create_bq_dt

$print "Creating DataFlow Pipeline to Ingest Data from [$SUBSCRIPTION_ID] into BQ [$BIGQUERY_DATASET].[$BIGQUERY_TABLE] (Last Step).."
deploy_dataflow







