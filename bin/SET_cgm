#! /bin/bash
DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
##################################################################################
#          Application Specific SETTINGS (Poject-Independent)
##################################################################################
export APPLICATION='dexcom'          # Application Label

#Region, Zone
export ZONE=${ZONE:-'us-west1-a'}
export REGION=${REGION:-'us-west1'} #Must be a valid location for App Engine creation
export NETWORK=${NETWORK:-'default'}

# BigQuery
export BIGQUERY_DATASET='datacloud'  # DataSet Name
export BIGQUERY_TABLE=$APPLICATION   # Table name for Application Data

# Cloud Function
export SOURCE_DIR="${DIR}/../py/dexcom"  # Cloud Function Directory - relative (main.py)
export SOURCE_ENTRY_POINT='dexcom_monitor' # Entry point for Cloud Function
export RUNTIME='python37' # The only properly Supported for Runtime Environment

# Cloud Scheduler
export INTERVAL=5 # Data Retrieval Interval in minutes

# Variable Names
export APPLICATION_ROLE=${APPLICATION}_role
export SERVICE_ACCOUNT_NAME=${APPLICATION}-service-account  #Runs Cloud Functions, DataFlow, Scheduler
export TOPIC_ID=$APPLICATION'-topic'       # Pub/Sub Topic for Device Data
export SUBSCRIPTION_ID=$APPLICATION'-sub'  # Pub/Sub Subscription for Application Data
export PUBSUB_COMMAND_TOPIC=$APPLICATION'-call' # Topic to Trigger Cloud Function
export SCHEDULER_JOB=$APPLICATION'-job'         # Scheduler Job Name To Trigger Topic
export DATAFLOW_JOB=$APPLICATION'_dataflow_job' # Dataflow Job name to Pipeline from Pub/Sub to BigQuery
#
#
#echo "Done ${BASH_SOURCE[0]}"